{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51bc594-0a99-4b4e-abca-95e7d5f0cb9e",
   "metadata": {},
   "source": [
    "# Downloading the Data using the Guardian Open Platform API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0264b7bc-1c13-4376-92a7-6ab373df638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e5501d-85bd-4e67-943c-455b071eb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define API key and endpoint\n",
    "API_KEY = '8692185a-3471-470c-99b7-9c798186ce67'  # change the key\n",
    "BASE_URL = 'https://content.guardianapis.com/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da089de3-42c9-467d-b61b-87c27ddacc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's set the parameters for fetching editorials\n",
    "params = {\n",
    "    'section': 'commentisfree',  # Section for editorials\n",
    "    'from-date': '2024-01-01',  # Start date\n",
    "    'to-date': '2024-11-30',  # End date\n",
    "    'page-size': 50,  # Number of articles per page (max 50 for free plan)\n",
    "    'show-fields': 'headline,body',  # Include headline and content\n",
    "    'api-key': API_KEY  # API key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e0d920-8574-40b6-b8ec-4c818bbd9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the function to fetch articles\n",
    "def fetch_articles(api_url, params, max_pages=100):\n",
    "    all_articles = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params['page'] = page\n",
    "        response = requests.get(api_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results = data.get('response', {}).get('results', [])\n",
    "            if not results:  # Stop if no articles are found\n",
    "                break\n",
    "            for article in results:\n",
    "                all_articles.append({\n",
    "                    'date_of_publication': article.get('webPublicationDate', ''),\n",
    "                    'headline': article['fields'].get('headline', ''),\n",
    "                    'content': article['fields'].get('body', '')\n",
    "                })\n",
    "            print(f\"Fetched {len(results)} articles from page {page}.\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a6b3b4-7f46-4073-9f7b-64031658a3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 articles from page 1.\n",
      "Fetched 50 articles from page 2.\n",
      "Fetched 50 articles from page 3.\n",
      "Fetched 50 articles from page 4.\n",
      "Fetched 50 articles from page 5.\n",
      "Fetched 50 articles from page 6.\n",
      "Fetched 50 articles from page 7.\n",
      "Fetched 50 articles from page 8.\n",
      "Fetched 50 articles from page 9.\n",
      "Fetched 50 articles from page 10.\n",
      "Fetched 50 articles from page 11.\n",
      "Fetched 50 articles from page 12.\n",
      "Fetched 50 articles from page 13.\n",
      "Fetched 50 articles from page 14.\n",
      "Fetched 50 articles from page 15.\n",
      "Fetched 50 articles from page 16.\n",
      "Fetched 50 articles from page 17.\n",
      "Fetched 50 articles from page 18.\n",
      "Fetched 50 articles from page 19.\n",
      "Fetched 50 articles from page 20.\n",
      "Fetched 50 articles from page 21.\n",
      "Fetched 50 articles from page 22.\n",
      "Fetched 50 articles from page 23.\n",
      "Fetched 50 articles from page 24.\n",
      "Fetched 50 articles from page 25.\n",
      "Fetched 50 articles from page 26.\n",
      "Fetched 50 articles from page 27.\n",
      "Fetched 50 articles from page 28.\n",
      "Fetched 50 articles from page 29.\n",
      "Fetched 50 articles from page 30.\n",
      "Fetched 50 articles from page 31.\n",
      "Fetched 50 articles from page 32.\n",
      "Fetched 50 articles from page 33.\n",
      "Fetched 50 articles from page 34.\n",
      "Fetched 50 articles from page 35.\n",
      "Fetched 50 articles from page 36.\n",
      "Fetched 50 articles from page 37.\n",
      "Fetched 50 articles from page 38.\n",
      "Fetched 50 articles from page 39.\n",
      "Fetched 50 articles from page 40.\n",
      "Fetched 50 articles from page 41.\n",
      "Fetched 50 articles from page 42.\n",
      "Fetched 50 articles from page 43.\n",
      "Fetched 50 articles from page 44.\n",
      "Fetched 50 articles from page 45.\n",
      "Fetched 50 articles from page 46.\n",
      "Fetched 50 articles from page 47.\n",
      "Fetched 50 articles from page 48.\n",
      "Fetched 50 articles from page 49.\n",
      "Fetched 50 articles from page 50.\n",
      "Fetched 50 articles from page 51.\n",
      "Fetched 50 articles from page 52.\n",
      "Fetched 50 articles from page 53.\n",
      "Fetched 50 articles from page 54.\n",
      "Fetched 50 articles from page 55.\n",
      "Fetched 50 articles from page 56.\n",
      "Fetched 50 articles from page 57.\n",
      "Fetched 50 articles from page 58.\n",
      "Fetched 50 articles from page 59.\n",
      "Fetched 50 articles from page 60.\n",
      "Fetched 50 articles from page 61.\n",
      "Fetched 50 articles from page 62.\n",
      "Fetched 50 articles from page 63.\n",
      "Fetched 50 articles from page 64.\n",
      "Fetched 50 articles from page 65.\n",
      "Fetched 50 articles from page 66.\n",
      "Fetched 50 articles from page 67.\n",
      "Fetched 50 articles from page 68.\n",
      "Fetched 50 articles from page 69.\n",
      "Fetched 50 articles from page 70.\n",
      "Fetched 50 articles from page 71.\n",
      "Fetched 50 articles from page 72.\n",
      "Fetched 50 articles from page 73.\n",
      "Fetched 50 articles from page 74.\n",
      "Fetched 50 articles from page 75.\n",
      "Fetched 50 articles from page 76.\n",
      "Fetched 50 articles from page 77.\n",
      "Fetched 50 articles from page 78.\n",
      "Fetched 50 articles from page 79.\n",
      "Fetched 50 articles from page 80.\n",
      "Fetched 50 articles from page 81.\n",
      "Fetched 50 articles from page 82.\n",
      "Fetched 50 articles from page 83.\n",
      "Fetched 50 articles from page 84.\n",
      "Fetched 50 articles from page 85.\n",
      "Fetched 50 articles from page 86.\n",
      "Fetched 50 articles from page 87.\n",
      "Fetched 50 articles from page 88.\n",
      "Fetched 50 articles from page 89.\n",
      "Fetched 50 articles from page 90.\n",
      "Fetched 3 articles from page 91.\n",
      "Failed to fetch data: 400, {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "# Now let's fetch the articles\n",
    "articles = fetch_articles(BASE_URL, params)\n",
    "\n",
    "# And, save them to a DataFrame\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55577faf-88cf-4add-8335-7965b9af9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to guardian_editorials_data.csv\n"
     ]
    }
   ],
   "source": [
    "# And save the data to a CSV file format\n",
    "df.to_csv('guardian_editorials_data.csv', index=False)\n",
    "print(\"Data saved to guardian_editorials_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f13b7-0245-421b-970d-1250215ab00f",
   "metadata": {},
   "source": [
    "# Initial Checks on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60defe88-d8b8-42d2-badd-d980407dd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "    date_of_publication                                           headline  \\\n",
      "0  2024-11-30T19:30:34Z  The Observer view: Shaky ceasefire is no victo...   \n",
      "1  2024-11-30T19:00:33Z  The Observer view: Ignore the stigma and tackl...   \n",
      "2  2024-11-30T18:00:33Z  Wicked would be fun and forgettable but for th...   \n",
      "3  2024-11-30T17:00:31Z  Feeding off anger, fuelled by Russia… Enter Că...   \n",
      "4  2024-11-30T16:00:32Z  What connects Huddersfield’s 1990s football st...   \n",
      "\n",
      "                                             content  \n",
      "0  <p>For the people of Lebanon, last week’s agre...  \n",
      "1  <p>‘I wanted them all to notice.” This is the ...  \n",
      "2  <p>The “war on woke” has a new target and her ...  \n",
      "3  <p>Politics in Romania can be a bloody busines...  \n",
      "4  <p>1994 was a vintage year for architecture. T...  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4503 entries, 0 to 4502\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   date_of_publication  4503 non-null   object\n",
      " 1   headline             4503 non-null   object\n",
      " 2   content              4503 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 105.7+ KB\n",
      "\n",
      "Summary Statistics:\n",
      "         date_of_publication  \\\n",
      "count                   4503   \n",
      "unique                  4433   \n",
      "top     2024-06-26T10:00:14Z   \n",
      "freq                       2   \n",
      "\n",
      "                                                 headline  \\\n",
      "count                                                4503   \n",
      "unique                                               4503   \n",
      "top     The Observer view: Shaky ceasefire is no victo...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                  content  \n",
      "count                                                4503  \n",
      "unique                                               4503  \n",
      "top     <p>For the people of Lebanon, last week’s agre...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's load the updated dataset\n",
    "file_path = 'data/guardian_editorials_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Let's inspect the first few rows\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# And, check the basic info about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "data.info()\n",
    "\n",
    "# Also check the summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23091d95-39cc-4f00-80cb-63ae2ebefdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "date_of_publication    0\n",
      "headline               0\n",
      "content                0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Now, let's check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# let's check for duplicate rows\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a954b7e-9248-4ec4-973a-95e83269e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Data Types:\n",
      "date_of_publication    object\n",
      "headline               object\n",
      "content                object\n",
      "dtype: object\n",
      "\n",
      "After converting 'date_of_publication' to datetime:\n",
      "date_of_publication    datetime64[ns, UTC]\n",
      "headline                            object\n",
      "content                             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's check current data types\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# And, convert 'date_of_publication' to datetime if applicable\n",
    "if 'date_of_publication' in data.columns:\n",
    "    data['date_of_publication'] = pd.to_datetime(data['date_of_publication'], errors='coerce')\n",
    "    print(\"\\nAfter converting 'date_of_publication' to datetime:\")\n",
    "    print(data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60cd1413-2129-4688-83e8-a5ba75e1de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total articles: 4503\n",
      "Unique headlines: 4503\n",
      "\n",
      "Headline Length Statistics:\n",
      "count    4503.000000\n",
      "mean       85.762825\n",
      "std        14.392470\n",
      "min        17.000000\n",
      "25%        77.000000\n",
      "50%        86.000000\n",
      "75%        96.000000\n",
      "max       134.000000\n",
      "Name: headline_length, dtype: float64\n",
      "\n",
      "Content Length Statistics:\n",
      "count     4503.000000\n",
      "mean      7165.015989\n",
      "std       2606.108669\n",
      "min       1880.000000\n",
      "25%       5454.000000\n",
      "50%       6983.000000\n",
      "75%       8473.000000\n",
      "max      48873.000000\n",
      "Name: content_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Let's count total articles and unique headlines\n",
    "print(f\"\\nTotal articles: {data.shape[0]}\")\n",
    "print(f\"Unique headlines: {data['headline'].nunique()}\")\n",
    "\n",
    "# Let's check headline length distribution\n",
    "data['headline_length'] = data['headline'].str.len()\n",
    "print(\"\\nHeadline Length Statistics:\")\n",
    "print(data['headline_length'].describe())\n",
    "\n",
    "# Also the content length distribution\n",
    "data['content_length'] = data['content'].str.len()\n",
    "print(\"\\nContent Length Statistics:\")\n",
    "print(data['content_length'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d8a857-f168-45b0-a2ba-e0a21343e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data saved to: data/clean_guardian_editorials_data.csv\n"
     ]
    }
   ],
   "source": [
    "# And, save cleaned data to a new CSV file\n",
    "cleaned_file_path = 'data/clean_guardian_editorials_data.csv'\n",
    "data.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"\\nCleaned data saved to: {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e2107-7b30-4236-a62b-6261377ca566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
