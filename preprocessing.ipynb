{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6740109b-d373-4948-a4fd-2235e5ae62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries for the preprocessing steps\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e03459-39a9-44e0-a48e-3dbebc88175a",
   "metadata": {},
   "source": [
    "# String Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616c0365-9425-4edb-8ad9-c73b8d9cd4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "         date_of_publication  \\\n",
      "0  2024-11-30 19:30:34+00:00   \n",
      "1  2024-11-30 19:00:33+00:00   \n",
      "2  2024-11-30 18:00:33+00:00   \n",
      "3  2024-11-30 17:00:31+00:00   \n",
      "4  2024-11-30 16:00:32+00:00   \n",
      "\n",
      "                                            headline  \\\n",
      "0  The Observer view: Shaky ceasefire is no victo...   \n",
      "1  The Observer view: Ignore the stigma and tackl...   \n",
      "2  Wicked would be fun and forgettable but for th...   \n",
      "3  Feeding off anger, fuelled by Russia… Enter Că...   \n",
      "4  What connects Huddersfield’s 1990s football st...   \n",
      "\n",
      "                                             content  headline_length  \\\n",
      "0  <p>For the people of Lebanon, last week’s agre...               98   \n",
      "1  <p>‘I wanted them all to notice.” This is the ...               85   \n",
      "2  <p>The “war on woke” has a new target and her ...               85   \n",
      "3  <p>Politics in Romania can be a bloody busines...               93   \n",
      "4  <p>1994 was a vintage year for architecture. T...               74   \n",
      "\n",
      "   content_length  \n",
      "0            5806  \n",
      "1            6672  \n",
      "2            8781  \n",
      "3            9107  \n",
      "4            4952  \n"
     ]
    }
   ],
   "source": [
    "# Let's load the dataset\n",
    "file_path = 'data/clean_guardian_editorials_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Let's check the first few rows for context\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Ensure all columns with text data are processed as strings\n",
    "data['headline'] = data['headline'].astype(str)\n",
    "data['content'] = data['content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a72259b-b865-4f2d-9f2c-f29791acc556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset Preview:\n",
      "         date_of_publication  \\\n",
      "0  2024-11-30 19:30:34+00:00   \n",
      "1  2024-11-30 19:00:33+00:00   \n",
      "2  2024-11-30 18:00:33+00:00   \n",
      "3  2024-11-30 17:00:31+00:00   \n",
      "4  2024-11-30 16:00:32+00:00   \n",
      "\n",
      "                                            headline  \\\n",
      "0  the observer view shaky ceasefire is no victor...   \n",
      "1  the observer view ignore the stigma and tackle...   \n",
      "2  wicked would be fun and forgettable but for th...   \n",
      "3  feeding off anger fuelled by russia enter clin...   \n",
      "4  what connects huddersfields s football stadium...   \n",
      "\n",
      "                                             content  headline_length  \\\n",
      "0  for the people of lebanon last weeks agreement...               98   \n",
      "1  i wanted them all to notice this is the title ...               85   \n",
      "2  the war on woke has a new target and her name ...               85   \n",
      "3  politics in romania can be a bloody business e...               93   \n",
      "4   was a vintage year for architecture the years...               74   \n",
      "\n",
      "   content_length  \n",
      "0            5806  \n",
      "1            6672  \n",
      "2            8781  \n",
      "3            9107  \n",
      "4            4952  \n"
     ]
    }
   ],
   "source": [
    "# Basic string operations\n",
    "# Let's remove redundant whitespace\n",
    "data['headline'] = data['headline'].apply(lambda x: \" \".join(x.split()))\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "# And convert to lowercase\n",
    "data['headline'] = data['headline'].str.lower()\n",
    "data['content'] = data['content'].str.lower()\n",
    "\n",
    "# Remove 'p' at the beginning of sentences in the content column\n",
    "data['content'] = data['content'].apply(lambda x: re.sub(r'^p', '', x.strip()))\n",
    "\n",
    "# Remove special characters, digits, and punctuation\n",
    "data['headline'] = data['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "# Check the cleaned dataset\n",
    "print(\"\\nCleaned Dataset Preview:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbde01-b121-4d40-a6d0-8a855375be2d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e67e21-1aba-4487-b61b-bb9fb2e3ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Dataset Preview:\n",
      "                                     headline_tokens  \\\n",
      "0  [the, observer, view, shaky, ceasefire, is, no...   \n",
      "1  [the, observer, view, ignore, the, stigma, and...   \n",
      "2  [wicked, would, be, fun, and, forgettable, but...   \n",
      "3  [feeding, off, anger, fuelled, by, russia, ent...   \n",
      "4  [what, connects, huddersfields, s, football, s...   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [for, the, people, of, lebanon, last, weeks, a...  \n",
      "1  [i, wanted, them, all, to, notice, this, is, t...  \n",
      "2  [the, war, on, woke, has, a, new, target, and,...  \n",
      "3  [politics, in, romania, can, be, a, bloody, bu...  \n",
      "4  [was, a, vintage, year, for, architecture, the...  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# Now, let's tokenize the headline and content column\n",
    "data['headline_tokens'] = data['headline'].apply(word_tokenize)\n",
    "data['content_tokens'] = data['content'].apply(word_tokenize)\n",
    "\n",
    "# And, check the tokenized columns\n",
    "print(\"\\nTokenized Dataset Preview:\")\n",
    "print(data[['headline_tokens', 'content_tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b548e-ea80-4926-9c72-0d084707e38d",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ae9291-6a39-4346-8a49-3fe76fae5d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset After Stopword Removal:\n",
      "                                     headline_tokens  \\\n",
      "0  [observer, view, shaky, ceasefire, victory, ne...   \n",
      "1  [observer, view, ignore, stigma, tackle, toxic...   \n",
      "2  [wicked, would, fun, forgettable, altright, wa...   \n",
      "3  [feeding, anger, fuelled, russia, enter, clin,...   \n",
      "4  [connects, huddersfields, football, stadium, n...   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [people, lebanon, last, weeks, agreement, halt...  \n",
      "1  [wanted, notice, title, new, report, hrefhttps...  \n",
      "2  [war, woke, new, target, name, wicked, witch, ...  \n",
      "3  [politics, romania, bloody, business, especial...  \n",
      "4  [vintage, year, architecture, years, popular, ...  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Let's remove stopwords\n",
    "data['headline_tokens'] = data['headline_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "data['content_tokens'] = data['content_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# And check the dataset after removing stopwords\n",
    "print(\"\\nDataset After Stopword Removal:\")\n",
    "print(data[['headline_tokens', 'content_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1db69c-f70d-440e-9b5c-a1134811712c",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b86ea4f7-c1e9-4376-b7ed-66108fbdadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset with Stemmed Tokens:\n",
      "                             headline_tokens_stemmed  \\\n",
      "0  [observ, view, shaki, ceasefir, victori, netan...   \n",
      "1  [observ, view, ignor, stigma, tackl, toxic, cy...   \n",
      "2  [wick, would, fun, forgett, altright, wage, da...   \n",
      "3  [feed, anger, fuell, russia, enter, clin, geor...   \n",
      "4  [connect, huddersfield, footbal, stadium, notr...   \n",
      "\n",
      "                              content_tokens_stemmed  \n",
      "0  [peopl, lebanon, last, week, agreement, halt, ...  \n",
      "1  [want, notic, titl, new, report, hrefhttpsasse...  \n",
      "2  [war, woke, new, target, name, wick, witch, we...  \n",
      "3  [polit, romania, bloodi, busi, especi, right, ...  \n",
      "4  [vintag, year, architectur, year, popular, pos...  \n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Now, let's apply stemming\n",
    "data['headline_tokens_stemmed'] = data['headline_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "data['content_tokens_stemmed'] = data['content_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# And check the dataset with stemmed tokens\n",
    "print(\"\\nDataset with Stemmed Tokens:\")\n",
    "print(data[['headline_tokens_stemmed', 'content_tokens_stemmed']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01ccdc-cf3b-43fb-a54f-eda6a89f78ad",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971486ef-1d0b-4093-9dc2-0d93fa7d8262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset with Lemmatized Tokens:\n",
      "                          headline_tokens_lemmatized  \\\n",
      "0  [observer, view, shaky, ceasefire, victory, ne...   \n",
      "1  [observer, view, ignore, stigma, tackle, toxic...   \n",
      "2  [wicked, would, fun, forgettable, altright, wa...   \n",
      "3  [feed, anger, fuel, russia, enter, clin, georg...   \n",
      "4  [connect, huddersfields, football, stadium, no...   \n",
      "\n",
      "                           content_tokens_lemmatized  \n",
      "0  [people, lebanon, last, weeks, agreement, halt...  \n",
      "1  [want, notice, title, new, report, hrefhttpsas...  \n",
      "2  [war, wake, new, target, name, wicked, witch, ...  \n",
      "3  [politics, romania, bloody, business, especial...  \n",
      "4  [vintage, year, architecture, years, popular, ...  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Now let's apply lemmatization\n",
    "data['headline_tokens_lemmatized'] = data['headline_tokens'].apply(lambda x: [lemmatizer.lemmatize(word, pos='v') for word in x])\n",
    "data['content_tokens_lemmatized'] = data['content_tokens'].apply(lambda x: [lemmatizer.lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "# And, check the dataset with lemmatized tokens\n",
    "print(\"\\nDataset with Lemmatized Tokens:\")\n",
    "print(data[['headline_tokens_lemmatized', 'content_tokens_lemmatized']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418ad98a-40d2-4c33-995a-bffe8804ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed data saved to: data/preprocessed_guardian_editorials_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Finally let's save the preprocessed data to a new CSV\n",
    "preprocessed_file_path = 'data/preprocessed_guardian_editorials_data.csv'\n",
    "data.to_csv(preprocessed_file_path, index=False)\n",
    "\n",
    "print(f\"\\nPreprocessed data saved to: {preprocessed_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6128d37-63e7-49e9-b044-97f4023c6289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
